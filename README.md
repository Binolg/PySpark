# Big Data Project Showcase

## Description

This repository serves as a showcase for a school project developed using PySpark for Big Data processing. It aims to demonstrate the practical application of PySpark and related technologies in handling large datasets.

This project allowed me to learn, apply, and highlight tools used for Big Data handling, and it was also my first time using Docker as a tool for development.

### Structure
The contents are divided into 6 stages:
- Performance Analysis: Comparing dataset formats on different parameters
- Pre-Process Data: Preliminary data cleaning and structuring
- Data Exploration: Generic exploration to understand the data and its variables
- Data Processing: Construction of the pipeline and its various operations
- Models Performance: Testing different models for various dataset iterations
- Deployment: Local deployment with Streamlit to showcase the best model in action

## Installation

To run the project locally using Docker, follow these steps:

1. Make sure you have Docker installed on your machine. If not, you can download and install it from [Docker's official website](https://www.docker.com/get-started).

2. Clone the repository to your local machine:
   ```bash
   git clone https://github.com/Binolg/PySpark.git
   ```
3. Adjust the docker-compose.yaml file to your preference.

4. Run the container:
   ```bash
   docker-compose up -d
   ```

## Technologies Used

- **PySpark**
- **Python**
- **Jupyter Notebooks**
- **Docker**
- **Git and GitHub**

## Source

The original dataset can be found in [Kaggle](https://www.kaggle.com/datasets/parisrohan/credit-score-classification)
